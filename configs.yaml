defaults:

  logdir: null
  traindir: null
  evaldir: null
  offline_traindir: ''
  offline_evaldir: ''
  seed: 0
  deterministic_run: False
  steps: 1e6
  parallel: False
  eval_every: 1e4
  eval_episode_num: 10
  log_every: 1e4
  reset_every: 0
  device: 'cuda:0'
  compile: True
  precision: 32
  debug: False
  video_pred_log: True

  # Environment
  task: 'dmc_walker_walk'
  size: [64, 64]
  envs: 1
  action_repeat: 2
  time_limit: 1000
  grayscale: False
  prefill: 2500
  reward_EMA: True

  # Model
  dyn_hidden: 512
  dyn_deter: 512
  dyn_stoch: 32
  dyn_discrete: 32
  dyn_rec_depth: 1
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1
  grad_heads: ['decoder', 'reward', 'cont']
  units: 512
  act: 'SiLU'
  norm: True
  encoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  decoder:
    {mlp_keys: '$^', cnn_keys: 'image', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
  actor:
    {layers: 2, dist: 'normal', entropy: 3e-4, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic:
    {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  reward_head:
    {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 0.0}
  cont_head:
    {layers: 2, loss_scale: 1.0, outscale: 1.0}
  dyn_scale: 0.5
  rep_scale: 0.1
  kl_free: 1.0
  weight_decay: 0.0
  unimix_ratio: 0.01
  initial: 'learned'

  # Training
  batch_size: 16
  batch_length: 64
  train_ratio: 512
  pretrain: 100
  model_lr: 1e-4
  opt_eps: 1e-8
  grad_clip: 1000
  dataset_size: 1000000
  opt: 'adam'

  # Entropy scheduling (auto-adjust exploration)
  entropy_schedule: 'constant'  # 'constant', 'linear', 'cosine'
  entropy_start: null  # If null, use actor.entropy
  entropy_end: null    # Target entropy at end of training

  # Checkpoint management
  checkpoint_every: 0      # Save checkpoint every N steps (0 = disabled)
  keep_best_n: 3           # Keep N best checkpoints by eval_return
  save_best_only: False    # Only save when eval_return improves

  # Behavior.
  discount: 0.997
  discount_lambda: 0.95
  imag_horizon: 15
  imag_gradient: 'dynamics'
  imag_gradient_mix: 0.0
  eval_state_mean: False

  # Exploration
  expl_behavior: 'greedy'
  expl_until: 0
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0
  disag_target: 'stoch'
  disag_log: True
  disag_models: 10
  disag_offset: 1
  disag_layers: 4
  disag_units: 400
  disag_action_cond: False

dmc_proprio:
  steps: 5e5
  action_repeat: 2
  envs: 4
  train_ratio: 512
  video_pred_log: false
  encoder: {mlp_keys: '.*', cnn_keys: '$^'}
  decoder: {mlp_keys: '.*', cnn_keys: '$^'}

dmc_vision:
  steps: 1e6
  action_repeat: 2
  envs: 4
  train_ratio: 512
  video_pred_log: true
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

crafter:
  task: crafter_reward
  step: 1e6
  action_repeat: 1
  envs: 1
  train_ratio: 512
  video_pred_log: true
  dyn_hidden: 1024
  dyn_deter: 4096
  units: 1024
  encoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: '$^', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 5, dist: 'onehot', std: 'none'}
  value: {layers: 5}
  reward_head: {layers: 5}
  cont_head: {layers: 5}
  imag_gradient: 'reinforce'

atari100k:
  steps: 4e5
  envs: 1
  action_repeat: 4
  train_ratio: 1024
  video_pred_log: true
  eval_episode_num: 100
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  stickey: False
  lives: unused
  noops: 30
  resize: opencv
  actions: needed
  time_limit: 108000

minecraft:
  task: minecraft_diamond
  step: 1e8
  parallel: True
  envs: 16
  # no eval
  eval_episode_num: 0
  eval_every: 1e4
  action_repeat: 1
  train_ratio: 16
  video_pred_log: true
  dyn_hidden: 1024
  dyn_deter: 4096
  units: 1024
  encoder: {mlp_keys: 'inventory|inventory_max|equipped|health|hunger|breath|obs_reward', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  decoder: {mlp_keys: 'inventory|inventory_max|equipped|health|hunger|breath', cnn_keys: 'image', cnn_depth: 96, mlp_layers: 5, mlp_units: 1024}
  actor: {layers: 5, dist: 'onehot', std: 'none'}
  value: {layers: 5}
  reward_head: {layers: 5}
  cont_head: {layers: 5}
  imag_gradient: 'reinforce'
  break_speed: 100.0
  time_limit: 36000

memorymaze:
  steps: 1e8
  action_repeat: 2
  actor: {dist: 'onehot', std: 'none'}
  imag_gradient: 'reinforce'
  task: 'memorymaze_9x9'

debug:
  debug: True
  pretrain: 1
  prefill: 1
  batch_size: 10
  batch_length: 20

# ドローン環境（PyBullet-Drones）
drone:
  task: drone_hover
  steps: 5e5
  action_repeat: 2
  envs: 1
  train_ratio: 512
  time_limit: 240  # 8秒 * 30Hz
  video_pred_log: false  # 状態ベースなのでビデオ予測なし
  # 状態ベースの観測（画像なし）
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}

drone_vision:
  task: drone_hover
  steps: 1e6
  action_repeat: 2
  envs: 1
  train_ratio: 512
  time_limit: 240
  video_pred_log: true
  # 画像ベースの観測
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

# 軌道追従タスク（Trajectory Following）
trajectory_circle:
  task: trajectory_circle
  steps: 5e5
  action_repeat: 2
  envs: 4
  train_ratio: 512
  time_limit: 300  # 10秒 * 30Hz
  video_pred_log: false
  # 状態ベース（位置＋ターゲット情報）
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}

trajectory_figure8:
  task: trajectory_figure8
  steps: 5e5
  action_repeat: 2
  envs: 1
  train_ratio: 512
  time_limit: 300
  video_pred_log: false
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}

trajectory_waypoints:
  task: trajectory_waypoints
  steps: 5e5
  action_repeat: 2
  envs: 1
  train_ratio: 512
  time_limit: 300
  video_pred_log: false
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}

# MilliSign-based Person Following (mmWave radar tag tracking)
millisign_follow:
  task: millisign_follow
  steps: 5e5
  action_repeat: 2
  envs: 4
  train_ratio: 512
  time_limit: 300  # 10秒 * 30Hz
  video_pred_log: false
  # 状態: ドローン(12) + レーダー検出(5) + 人速度(3) + ターゲット情報(3) = 23次元
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}

millisign_approach:
  task: millisign_approach
  steps: 5e5
  action_repeat: 2
  envs: 4
  train_ratio: 512
  time_limit: 300
  video_pred_log: false
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}

# MilliSign v2 - Improved version with curriculum learning
millisignv2:
  task: millisignv2_follow
  steps: 5e5
  action_repeat: 1  # Less action repeat for finer control
  envs: 4
  batch_size: 64  # Increased for RTX 5060 Ti (16GB VRAM)
  train_ratio: 512
  time_limit: 360  # 12 seconds (longer episodes)
  video_pred_log: false
  prefill: 5000  # More prefill for exploration
  # State: drone(12) + radar(5) + velocity(3) + target(3) + curriculum(1) = 24 dims
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}
  # More exploration
  actor: {layers: 2, dist: 'normal', entropy: 1e-3, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}

# MilliSign v2 with longer training
millisignv2_long:
  task: millisignv2_follow
  steps: 1e6
  action_repeat: 1
  envs: 4
  train_ratio: 512
  time_limit: 360
  video_pred_log: false
  prefill: 5000
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}
  actor: {layers: 2, dist: 'normal', entropy: 1e-3, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}

# MilliSign v2 Improved - Better tracking performance
millisignv2_improved:
  task: millisignv2_follow
  steps: 1e6
  action_repeat: 1
  envs: 4
  batch_size: 64
  batch_length: 64
  train_ratio: 512
  time_limit: 480  # Longer episodes (16 sec)
  video_pred_log: false
  prefill: 10000  # More prefill for better initial exploration
  discount: 0.995  # Slightly lower discount for faster learning
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}
  # More exploration with higher entropy
  actor: {layers: 3, dist: 'normal', entropy: 3e-3, unimix_ratio: 0.02, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}

# MilliSign v3 - Enhanced reward function and curriculum
millisignv3:
  task: millisignv3_follow
  steps: 1e6
  action_repeat: 1
  envs: 4
  batch_size: 64
  batch_length: 64
  train_ratio: 512
  time_limit: 480  # 16 seconds
  video_pred_log: false
  prefill: 10000
  discount: 0.995
  # State: drone(12) + radar(5) + velocity(3) + target(3) + curriculum(1) + streak(1) = 25 dims
  encoder: {mlp_keys: 'state', cnn_keys: '$^'}
  decoder: {mlp_keys: 'state', cnn_keys: '$^'}
  # Higher entropy for better exploration in fast scenarios
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.02, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}

# MilliSign v4 - State history, yaw alignment, strong distance reward
millisignv4:
  task: millisignv4_follow
  steps: 1.5e6  # Longer training for complex state space
  action_repeat: 1
  envs: 4
  batch_size: 32  # Reduced for memory
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 15000
  discount: 0.995
  # State: drone(72) + radar(5) + history(25) + vel(3) + pred_dir(2) + rel_vel(3) + target(3) + heading(2) + curriculum(1) + streak(1) + lost(1) = 118 dims
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  # Medium world model (fits in 16GB VRAM)
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  # Actor/critic
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.02, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  # Longer imagination horizon for better planning
  imag_horizon: 20

# MilliSign v4 Finetune - Lower entropy, lower LR for stability
millisignv4_finetune:
  task: millisignv4_follow
  steps: 2e6  # Continue training
  action_repeat: 1
  envs: 4
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 0  # No prefill, continue from checkpoint
  discount: 0.995
  # State: same as v4
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  # Lower entropy (1e-3 vs 5e-3) and lower LR (3e-5 vs 1e-4) for fine-tuning
  actor: {layers: 3, dist: 'normal', entropy: 1e-3, unimix_ratio: 0.01, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v5 - With entropy scheduling and checkpoint management
millisignv5:
  task: millisignv4_follow
  steps: 1.5e6
  action_repeat: 1
  envs: 4
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 15000
  discount: 0.995
  # State: same as v4 (118 dims)
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  # Entropy scheduling: start high, end low
  entropy_schedule: 'linear'
  entropy_start: 3e-3    # Start with exploration
  entropy_end: 5e-4      # End with exploitation
  # Checkpoint management
  checkpoint_every: 50000   # Save every 50k steps
  keep_best_n: 5            # Keep top 5 models
  # Actor/critic with moderate initial entropy
  actor: {layers: 3, dist: 'normal', entropy: 2e-3, unimix_ratio: 0.02, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v6 - Fast mode finetuning (speed 0.8-1.2)
millisignv6_fast:
  task: millisignv4_follow_fast
  steps: 500000
  action_repeat: 1
  envs: 4
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 20000  # Collect data at high speed first
  discount: 0.995
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  # Higher entropy for exploration in fast scenarios
  entropy_schedule: 'linear'
  entropy_start: 5e-3
  entropy_end: 1e-3
  checkpoint_every: 25000
  keep_best_n: 5
  # Lower LR for finetuning stability
  actor: {layers: 3, dist: 'normal', entropy: 3e-3, unimix_ratio: 0.02, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 5e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 5e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v7 - Position-based targeting (train from scratch)
millisignv7:
  task: millisignv4_follow
  steps: 1000000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.995
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: 'linear'
  entropy_start: 5e-3
  entropy_end: 1e-3
  checkpoint_every: 25000
  keep_best_n: 5
  actor: {layers: 3, dist: 'normal', entropy: 3e-3, unimix_ratio: 0.02, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 8e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 8e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v9 - Distance-proportional reward
# Key changes: Reward inversely proportional to distance, always better to be closer
millisignv9:
  task: millisignv4_follow
  steps: 500000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.995
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: 'linear'
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 25000
  keep_best_n: 5
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.03, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v12 - Potential-Based Reward Shaping (PBRS)
# Based on: "Revisiting Sparse Rewards for Goal-Reaching RL" (arXiv 2407.00324)
# Theory: PBRS guarantees policy invariance (no reward hacking)
# Components:
# 1. PBRS: F = γΦ(s') - Φ(s) where Φ = -distance
# 2. Goal bonus: +20 at 0.5m, +5 at 1.0m
# 3. Time penalty: -0.5/step (minimum time formulation)
millisignv12:
  task: millisignv4_follow
  steps: 500000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: 'linear'
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 25000
  keep_best_n: 5
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.03, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v13 - Continuous Curriculum (A案)
# Changes from v12:
# - Continuous curriculum: linear interpolation from 0.1 to 1.2 over 400k steps
# - No stationary phase: starts with moving target (speed 0.1)
# - Smoother difficulty progression (no sudden jumps)
millisignv13:
  task: millisignv4_follow
  steps: 500000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: 'linear'
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.03, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v14 - Goal-Forcing with Safety
# Problem: v13 drone tracks person but stays 2-5m away, not reaching 0.5m target
# Key changes:
# 1. Strong position reward at goal (+15.0 at <0.5m), penalty when far (forces convergence)
# 2. Detection REQUIRED for full reward (prevents blind flying)
# 3. Velocity penalty for speeds > 3m/s (safety)
# 4. Stagnation penalty (must move when far from goal)
# 5. Continuous curriculum from v13 (linear 0.1-1.2 m/s over 400k steps)
millisignv14:
  task: millisignv4_follow
  reward_version: v14
  steps: 500000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: 'linear'
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.03, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v11 - Speed-focused reward design
# Problem: v10 drone too slow to keep up with moving target
# Key changes:
# 1. Velocity toward target reward (+8.0 per m/s)
# 2. Stronger approach reward (15.0 per meter, tripled from v10)
# 3. Distance-based speed requirement (must move fast when far)
# 4. Person speed matching bonus
# 5. Removed time penalty (was discouraging movement)
millisignv11:
  task: millisignv4_follow
  steps: 500000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.995
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: 'linear'
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 25000
  keep_best_n: 5
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.03, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v10 - Research-informed reward design
# Based on: Dream to Fly, DreamerNav, UAV Target Tracking papers
# Key changes:
# 1. Asymmetric distance reward (retreat penalty > approach reward)
# 2. Stagnation penalty (penalize not moving when far from target)
# 3. Target reach bonus (sparse reward for being close)
# 4. Small time penalty to encourage efficiency
millisignv10:
  task: millisignv4_follow
  steps: 500000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.995
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: 'linear'
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 25000
  keep_best_n: 5
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.03, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v8 - Simplified reward function (position-dominant)
# Key changes: Remove lazy hovering incentive, strongly reward reaching target
millisignv8:
  task: millisignv4_follow
  steps: 500000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 512
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.995
  encoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: 'state', cnn_keys: '$^', mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  # Higher initial entropy for active exploration
  entropy_schedule: 'linear'
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 25000
  keep_best_n: 5
  # Higher entropy and LR for active behavior
  actor: {layers: 3, dist: 'normal', entropy: 5e-3, unimix_ratio: 0.03, std: 'learned', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v15 - Simple reward design
# Key changes from v14:
# 1. Only 3 reward terms (distance, detection bonus, success bonus)
# 2. Detection failure = no bonus (not multiplier penalty)
# 3. Unified reward scale (-1 to +1.8)
# 4. Lower train_ratio to prevent overfitting
millisignv15:
  task: millisignv4_follow
  reward_version: v15
  steps: 200000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

# MilliSign v16 - No curriculum, always hard conditions
# Key changes from v15:
# 1. No curriculum - always challenging patterns (walk_random, walk_zigzag, walk_sudden)
# 2. Always fast person movement (0.6-1.2x speed)
# 3. Varied starting positions (1.5-3.5m, -1.5-1.5m)
millisignv16:
  task: millisignv4_follow
  reward_version: v15
  steps: 200000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

millisignv17:
  task: millisignv4_follow
  reward_version: v17
  steps: 200000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

millisignv18:
  task: millisignv4_follow
  reward_version: v18
  steps: 200000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

millisignv19:
  task: millisignv4_follow
  reward_version: v19
  steps: 200000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

millisignv20:
  task: millisignv4_follow
  reward_version: v20
  steps: 200000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

millisignv21:
  task: millisignv4_follow
  reward_version: v21
  steps: 200000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

millisignv22:
  task: millisignv4_follow
  reward_version: v21
  stationary_target: true
  steps: 100000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 480
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20

millisignv23:
  task: millisignv4_follow
  reward_version: v21
  stationary_target: true
  goal_termination: true
  steps: 100000
  action_repeat: 1
  envs: 8
  batch_size: 32
  batch_length: 64
  train_ratio: 256
  time_limit: 1000
  video_pred_log: false
  prefill: 10000
  discount: 0.99
  encoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  decoder: {mlp_keys: "state", cnn_keys: "$^", mlp_layers: 4, mlp_units: 512}
  dyn_hidden: 512
  dyn_deter: 512
  units: 512
  entropy_schedule: "linear"
  entropy_start: 8e-3
  entropy_end: 1e-3
  checkpoint_every: 10000
  keep_best_n: 5
  actor: {layers: 3, dist: "normal", entropy: 5e-3, unimix_ratio: 0.03, std: "learned", min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic: {layers: 3, dist: "symlog_disc", slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 1e-4, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  imag_horizon: 20
